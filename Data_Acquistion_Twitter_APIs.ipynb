{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XisSZuAg-P30huMmCyOD8a93r26sX4YQ",
      "authorship_tag": "ABX9TyMjWHlccNNwCwcc4Ls0+xFm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmaCOo/ADS509_Text_Mining_Final_Project/blob/main/Data_Acquistion_Twitter_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ADS 509_TEXT MINING_Final_Project : Data Acquisition with Twitter APIs**\n",
        "\n",
        "###**Emma Oo**\n",
        "\n",
        "###This notebook covers the API pulls from Twitter Followers of Each Restaurant Chains.  The restaurant chains involved in this project are \n",
        "\n",
        "\n",
        "\n",
        "1.   Burger King\n",
        "2.   Jack In The Box\n",
        "3.   Inn-N-Out\n",
        "4.   Chick-Fil-A\n",
        "5.   Sonic Drive In\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EIuBnaxme0eB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spBcVe4Ctkg5",
        "outputId": "c38218b8-5fbe-4592-cc97-0ebb77c6bbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/tweepy/tweepy.git\n",
            "  Cloning https://github.com/tweepy/tweepy.git to /tmp/pip-req-build-90rx_e_m\n",
            "  Running command git clone -q https://github.com/tweepy/tweepy.git /tmp/pip-req-build-90rx_e_m\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.1) (3.2.0)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.1) (2.28.1)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.1) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "#install tweepy higher than 4.0\n",
        "!pip install git+https://github.com/tweepy/tweepy.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tweepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7cU6pZjDChB",
        "outputId": "b9a20267-7ece-4755-d104-3fe630304c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tweepy\n",
            "Version: 4.10.1\n",
            "Summary: Twitter library for Python\n",
            "Home-page: https://www.tweepy.org/\n",
            "Author: Joshua Roesslein\n",
            "Author-email: tweepy@googlegroups.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: requests-oauthlib, requests, oauthlib\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = 'key' \n",
        "api_key_secret = 'key'\n",
        "bearer_token = 'key'\n",
        "access_token = 'key'\n",
        "access_token_secret = 'key'\n"
      ],
      "metadata": {
        "id": "9OGCdwSqtvWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate\n",
        "import tweepy\n",
        "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)"
      ],
      "metadata": {
        "id": "IvNkcZGIurBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
      ],
      "metadata": {
        "id": "t5hfwvOEAHdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the twitter section\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "import os\n",
        "import datetime\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict, Counter"
      ],
      "metadata": {
        "id": "hzojrtF-bag4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://blog.quantinsti.com/twitter-api-v2/\n",
        "(good one I follow)\n",
        "\n",
        "https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet\n"
      ],
      "metadata": {
        "id": "rpH4bSv3LiHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chains = dict()\n",
        "for handle in ['McDonalds', 'BurgerKing'] :\n",
        "  user_obj = client.get_user(username=handle,user_fields=[\"public_metrics\"]) \n",
        "  chains[handle] = (user_obj.data.id,\n",
        "                     handle, user_obj.data.public_metrics['followers_count'])\n",
        "  \n",
        "for chain, data in chains.items() :\n",
        "  print(f\"It would take {data[2]/(1000*15*4):.2f} hours to pull all {data[2]} followers for {chain}. \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRmIwxIQTdf-",
        "outputId": "e51a92df-a273-4203-8af8-b3340eead01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It would take 77.80 hours to pull all 4668182 followers for McDonalds. \n",
            "It would take 33.99 hours to pull all 2039380 followers for BurgerKing. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.isdir(\"twitter\") : \n",
        "  #shutil.rmtree(\"twitter/\") \n",
        "  os.mkdir(\"twitter\")"
      ],
      "metadata": {
        "id": "-deyALygUkWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.get_user(id=user_obj.data.id,\n",
        "                          user_fields=[\"created_at\",\"description\",\"location\",\n",
        "                                       \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\n",
        "                                       \"verified\",\"public_metrics\"])"
      ],
      "metadata": {
        "id": "TXXalb2fX40w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**BURGER KING API PULL FOR FOLLOWERS AND DESCRIPTIONS**"
      ],
      "metadata": {
        "id": "0P91SrUBm4QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handles = ['BurgerKing']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit= 20):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count','like_count', 'description']\n",
        "    columns_2 = ['ID']\n",
        "    bgk = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    bgk_users = pd.DataFrame(followers_data[handle], columns = columns_2)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    bgk.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    bgk_users.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0J3G3nHm8y9",
        "outputId": "fe0abce4-eb38-403d-bc62-9aad25537535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '6QSS6CLEGGSHGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'T4SNT93NAGSHGZZZ', 'previous_token': 'RM4K5PQVFF3EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'G7CHGS6358SHGZZZ', 'previous_token': 'SSR866SILF3EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '0G4CTKC4V4S1GZZZ', 'previous_token': '7MNMFQPVQN3EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'DO686IFNP8S1GZZZ', 'previous_token': 'IS5ROVC00R3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CLIVT30RJ8S1GZZZ', 'previous_token': '0JN1GNGN6N3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'D1G2GVC9BSS1GZZZ', 'previous_token': '9MQGE3V5CN3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '724LPOB54CS1GZZZ', 'previous_token': '44FC2CRTK33UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'QR25KGSDV0RHGZZZ', 'previous_token': 'M670AIKSRJ3UEZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 508 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '1VRB2GO8P8RHGZZZ', 'previous_token': 'L9N9NSC30V4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '9FCFVTETJKRHGZZZ', 'previous_token': 'MT8NIBGA6R4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'ST8KS8CMBSRHGZZZ', 'previous_token': 'F85I82P2CB4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'ANSIVS9O40RHGZZZ', 'previous_token': 'AP013E3EK34EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '024Q62UITCR1GZZZ', 'previous_token': 'SKGJU8MSRV4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '5OI3B3KTO8R1GZZZ', 'previous_token': 'CQNIC31S2J4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '36IR9TDLJ0R1GZZZ', 'previous_token': '6SORCNBF7N4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '3P1I7DDGD8R1GZZZ', 'previous_token': 'NQSL2GIACV4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'PTJHOACT80R1GZZZ', 'previous_token': '7RM32UIUIN4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'GTDAFENQ2KR1GZZZ', 'previous_token': '59LPDRR8NV4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'RM14RB26SSQHGZZZ', 'previous_token': '3L31VA09TB4UEZZZ'}\n",
            "0:08:38.475400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bgk_df = bgk[bgk[\"description\"].str.strip().str.len()>0]\n",
        "bgk_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMK4vHyYqOKs",
        "outputId": "0b2adf62-830f-4e9c-c560-bd21e1dca9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7011, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check any duplicates\n",
        "print((bgk_df.duplicated()).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUL31pgPqV7f",
        "outputId": "a726e0df-4cb9-4389-f55b-899d24dc9296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**In-N-Out Burger**"
      ],
      "metadata": {
        "id": "_7fKgD5Rtotv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handles = ['innoutburger']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit= 20):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count','like_count', 'description']\n",
        "    columns_2 = ['ID']\n",
        "    inn = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    inn_users = pd.DataFrame(followers_data[handle], columns = columns_2)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    inn.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    inn_users.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mux5cofstx56",
        "outputId": "c4c18a00-141b-47af-9b72-7852cf2fe355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 649 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'G2LKH2O32CSHGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'BSFO4O5JIGS1GZZZ', 'previous_token': 'M4SOJF85TN3EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '3NQOS72C2GS1GZZZ', 'previous_token': 'RNBVAS2EDF3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'D6O3B5H9N8RHGZZZ', 'previous_token': 'C0QIOM66TF3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '68F4CP543GRHGZZZ', 'previous_token': 'GBC8GB7B8N4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '9BUL27QDIKR1GZZZ', 'previous_token': '70PAK2BASF4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '2RL47B5J6OR1GZZZ', 'previous_token': 'FJQL6D68DB4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'JNCP200VSGQHGZZZ', 'previous_token': '40HPHQJ8P74UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'VCG86RC0GOQHGZZZ', 'previous_token': '9H1T8PNE3F5EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'GE6TIVLO2GQHGZZZ', 'previous_token': 'GSB21AKDF75EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6OO7LQH1G0Q1GZZZ', 'previous_token': '4215623ETF5EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'R1R035V13CQ1GZZZ', 'previous_token': 'AHM1R0NPFV5UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6CV71QI7Q8PHGZZZ', 'previous_token': 'G36SDDHMSJ5UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'FQIEQHETF0PHGZZZ', 'previous_token': 'SJHQS46T5N6EEZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 893 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '7ULDD3531SPHGZZZ', 'previous_token': '2RVV2SH6GV6EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'UB4DL52HKSP1GZZZ', 'previous_token': '6BK4KPKVU36EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CFLTCQKT98P1GZZZ', 'previous_token': '56BN0RMHB36UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'P7LJQ7DIUOOHGZZZ', 'previous_token': 'NPDLLFTSMN6UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'B0SF7VURM0OHGZZZ', 'previous_token': '5G7NT4QL177EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'S6DN6EQJHGOHGZZZ', 'previous_token': '1JSK5QHG9V7EEZZZ'}\n",
            "0:25:52.421812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inn_df = inn[inn[\"description\"].str.strip().str.len()>0]\n",
        "inn_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8_0NHvV1ZiS",
        "outputId": "888fa27f-1190-4545-bb6a-726abefe3cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9877, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**JACK IN THE BOX**"
      ],
      "metadata": {
        "id": "TPSLB3g14udQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handles = ['JackBox']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit= 20):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count','like_count', 'description']\n",
        "    columns_2 = ['ID']\n",
        "    jkb = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    jkb_users = pd.DataFrame(followers_data[handle], columns = columns_2)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    jkb.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    jkb_users.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chqe0avF1j4Y",
        "outputId": "a4de7f7c-cf68-4783-aa60-388dc3f849d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'P2SI51GR60HHGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'N9NJSCACGFVHEZZZ', 'previous_token': 'MTAA0J7EPVEEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'A3R25PNLON81EZZZ', 'previous_token': 'V0GLV3L8FO0EGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'KLH7FS4D9V51EZZZ', 'previous_token': 'O913OU0R7KNUGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'LNOG027QRJ4HEZZZ', 'previous_token': 'FQIT077RM0QUGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'S4IB8UO30II1EZZZ', 'previous_token': 'QBGLCH874CREGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '95JQIOSA8HEHEZZZ', 'previous_token': '9VVJANUT19EEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6NUGI6P4RKJHEZZZ', 'previous_token': 'M99MD7RONEHEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CV3R0UTNONN1CZZZ', 'previous_token': 'JDF3J3FF7BCEGZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 373 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'RQIOIN0U3EUHCZZZ', 'previous_token': 'P231GPAFF08UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '48SIQ8AK0IBHCZZZ', 'previous_token': '8G89GMK7051UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'S4QGHCAL75U1CZZZ', 'previous_token': 'AQ3S81SQ2LKUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'T06EGQ82U9EHCZZZ', 'previous_token': 'DGCF7P0HOU1UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'S7KH2E4R2DCHCZZZ', 'previous_token': 'NJRDTCVK32HEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'RGQA7SV7NPC1CZZZ', 'previous_token': 'TEFNC7BITIJEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'NRUJ0G7IMP0HCZZZ', 'previous_token': 'V75C711S86JUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'I9DIR91M6OK1CZZZ', 'previous_token': 'T4T4JOM296VEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'NIR17071AC91CZZZ', 'previous_token': 'K7VAK1BEPBBUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6KMO6P5KTC01CZZZ', 'previous_token': 'C77TJAQGLRMUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'PHUF4C3MC7K1AZZZ', 'previous_token': '1VTS9FS62JVUIZZZ'}\n",
            "0:06:24.000557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jkb_df = jkb[jkb[\"description\"].str.strip().str.len()>0]\n",
        "jkb_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEzxJJ2N3snY",
        "outputId": "9d3cb25f-3463-412f-a921-a439a355c570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13800, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**CHICK-FIL-A**"
      ],
      "metadata": {
        "id": "RDyAyKae442L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handles = ['ChickfilA']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit= 20):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count','like_count', 'description']\n",
        "    columns_2 = ['ID']\n",
        "    chick = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    chick_users = pd.DataFrame(followers_data[handle], columns = columns_2)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    chick.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    chick_users.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oTu9oF6A7H8",
        "outputId": "d454f78c-7f8d-4903-a55a-75030ad76ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'V4JU4EG8S8QHGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '4H8GQMP70SN1GZZZ', 'previous_token': 'RQQD0UTQ3V5EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'QO8KPULTUCJHGZZZ', 'previous_token': 'UPHICDJ4VB8UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '70MHE5DT20H1GZZZ', 'previous_token': 'OGS0PO531JCEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'NU21J1T9M8F1GZZZ', 'previous_token': 'MG4JE6CAU3EUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'TAAKF0GJ5ODHGZZZ', 'previous_token': 'BFQJ6L679RGUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '4U3ECDNDM8D1GZZZ', 'previous_token': 'I6EMH68GQBIEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '4VI9C4MHJGD1GZZZ', 'previous_token': 'KFQTK18Q9NIUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '7LLC6DL2GSD1GZZZ', 'previous_token': 'S1IEHUHECFIUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'FB0R61JNT8CHGZZZ', 'previous_token': 'KGOCR133F3IUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '8DOE682AN091GZZZ', 'previous_token': '6UNCM1EQ2VJEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'V09LEM5ATO4HGZZZ', 'previous_token': 'S4IG51N28VMUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'E8104O8F5C21GZZZ', 'previous_token': '1BONO98V2JREEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '7O3LARNS1FUHEZZZ', 'previous_token': '92EJAV3TQNTUEZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 893 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '7EFHC9NT3VP1EZZZ', 'previous_token': 'EUQNGB3T1C1UGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '1O3PDJCPAVJHEZZZ', 'previous_token': 'NSDTF0DMS06UGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'K170RNRIIVDHEZZZ', 'previous_token': '645TVP0TLKCEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'MAF5LBR3EN8HEZZZ', 'previous_token': '1LNPSSELD0IEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '3HB7J73QL75HEZZZ', 'previous_token': '83DMJ6LLHSNEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'RF1DH67CNJ2HEZZZ', 'previous_token': 'S49ES0CVAOQEGZZZ'}\n",
            "0:15:03.707168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chick_df = chick[chick[\"description\"].str.strip().str.len()>0]\n",
        "chick_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0WBC6CDPW8Q",
        "outputId": "5818eac4-be7b-474a-a857-ff9129da4378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14093, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**SONIC DRIVE IN**"
      ],
      "metadata": {
        "id": "FzPUTBED4-7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "handles = ['sonicdrivein']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit= 20):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count','like_count', 'description']\n",
        "    columns_2 = ['ID']\n",
        "    sonic = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    sonic_users = pd.DataFrame(followers_data[handle], columns = columns_2)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    sonic.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    sonic_users.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpbj-HBaLxK8",
        "outputId": "c350efa7-36a5-4d8d-d761-3f757cd4c68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '4HLM6BJIV8RHGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'JEKRKPK30KR1GZZZ', 'previous_token': 'JNRH80LD0N4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '7AIB3FIP54Q1GZZZ', 'previous_token': '6L7KC2C1VB4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '7HQ7AEKO8CPHGZZZ', 'previous_token': '71VF4T9SQV5UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'IKD2H3I7I8P1GZZZ', 'previous_token': '3A7SQOJ9NJ6EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'LM4F32B2HGOHGZZZ', 'previous_token': '1HD8LVVEDN6UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'UARKFDHQJCNHGZZZ', 'previous_token': 'TNH9BRI2EN7EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'QJQHG44VN4MHGZZZ', 'previous_token': 'M9QQA0T2CN8EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'V14PU4JLJSLHGZZZ', 'previous_token': '2M85QGR88R9EEZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 428 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'FEA15BCJDOKHGZZZ', 'previous_token': 'KD1GF5H4C7AEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '7JC3CKV3H4JHGZZZ', 'previous_token': 'BBNDNEKJIFBEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'HV15H7TSGGIHGZZZ', 'previous_token': 'PSJGFI85EVCEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '9POUU6OHQOHHGZZZ', 'previous_token': '649RSILVFJDEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '98E0OI7UGSH1GZZZ', 'previous_token': 'CJI26A8L5BEEEZZZ'}\n",
            "{'result_count': 999, 'next_token': 'H9C0ON7B7OGHGZZZ', 'previous_token': 'Q4OV4B49F3EUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'LQK5MQT3NSFHGZZZ', 'previous_token': 'URFQ4VPJO7FEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '3715UDNIHCF1GZZZ', 'previous_token': '6CQONVAK87GEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'LJJNTL2ONSEHGZZZ', 'previous_token': '58CTD3GIEJGUEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'QJV7ND3R8GEHGZZZ', 'previous_token': 'PQGILQTA83HEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '5CV3S9PUL8E1GZZZ', 'previous_token': '2AIRRG45NFHEEZZZ'}\n",
            "0:07:18.665996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sonic_df = sonic[sonic[\"description\"].str.strip().str.len()>0]\n",
        "sonic_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWsaqLKGTiFh",
        "outputId": "b4e986df-8700-4c24-d390-f54c35e3907c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11395, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check any duplicates\n",
        "print((bgk_df.duplicated()).sum())\n",
        "print((inn_df.duplicated()).sum())\n",
        "print((jkb_df.duplicated()).sum())\n",
        "print((chick_df.duplicated()).sum())\n",
        "print((sonic_df.duplicated()).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O4hzuB93432",
        "outputId": "7e7625b5-79d0-49c5-8d06-ab5a1a19acc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export as csv files\n",
        "inn_df.to_csv('innout.csv',index=False)\n",
        "jkb_df.to_csv('jack.csv',index=False)\n",
        "bgk_df.to_csv('burgerk.csv', index=False)\n",
        "chick_df.to_csv('chickfila.csv', index=False)\n",
        "sonic_df.to_csv('sonic.csv',index=False)"
      ],
      "metadata": {
        "id": "PduwVJwS91oL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}