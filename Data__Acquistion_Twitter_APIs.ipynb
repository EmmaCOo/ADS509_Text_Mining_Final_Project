{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XisSZuAg-P30huMmCyOD8a93r26sX4YQ",
      "authorship_tag": "ABX9TyOTxHBTnE8Ct1uu/lgvuyKR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmaCOo/ADS509_Text_Mining_Final_Project/blob/main/Data__Acquistion_Twitter_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ADS 509_TEXT MINING_Final_Project : Data Acquisition with Twitter APIs**\n",
        "\n",
        "###**Emma Oo**\n"
      ],
      "metadata": {
        "id": "EIuBnaxme0eB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spBcVe4Ctkg5",
        "outputId": "c38218b8-5fbe-4592-cc97-0ebb77c6bbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/tweepy/tweepy.git\n",
            "  Cloning https://github.com/tweepy/tweepy.git to /tmp/pip-req-build-90rx_e_m\n",
            "  Running command git clone -q https://github.com/tweepy/tweepy.git /tmp/pip-req-build-90rx_e_m\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.1) (3.2.0)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.1) (2.28.1)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.1) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "#install tweepy higher than 4.0\n",
        "!pip install git+https://github.com/tweepy/tweepy.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tweepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7cU6pZjDChB",
        "outputId": "b9a20267-7ece-4755-d104-3fe630304c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tweepy\n",
            "Version: 4.10.1\n",
            "Summary: Twitter library for Python\n",
            "Home-page: https://www.tweepy.org/\n",
            "Author: Joshua Roesslein\n",
            "Author-email: tweepy@googlegroups.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: requests-oauthlib, requests, oauthlib\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = 'key' \n",
        "api_key_secret = 'key'\n",
        "bearer_token = 'key'\n",
        "access_token = 'key'\n",
        "access_token_secret = 'key'\n"
      ],
      "metadata": {
        "id": "9OGCdwSqtvWC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate\n",
        "import tweepy\n",
        "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)"
      ],
      "metadata": {
        "id": "IvNkcZGIurBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
      ],
      "metadata": {
        "id": "t5hfwvOEAHdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the twitter section\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "import os\n",
        "import datetime\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict, Counter"
      ],
      "metadata": {
        "id": "hzojrtF-bag4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://blog.quantinsti.com/twitter-api-v2/\n",
        "(good one I follow)\n",
        "\n",
        "https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet\n"
      ],
      "metadata": {
        "id": "rpH4bSv3LiHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chains = dict()\n",
        "for handle in ['McDonalds', 'BurgerKing'] :\n",
        "  user_obj = client.get_user(username=handle,user_fields=[\"public_metrics\"]) \n",
        "  chains[handle] = (user_obj.data.id,\n",
        "                     handle, user_obj.data.public_metrics['followers_count'])\n",
        "  \n",
        "for chain, data in chains.items() :\n",
        "  print(f\"It would take {data[2]/(1000*15*4):.2f} hours to pull all {data[2]} followers for {chain}. \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRmIwxIQTdf-",
        "outputId": "e51a92df-a273-4203-8af8-b3340eead01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It would take 77.80 hours to pull all 4668182 followers for McDonalds. \n",
            "It would take 33.99 hours to pull all 2039380 followers for BurgerKing. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.isdir(\"twitter\") : \n",
        "  #shutil.rmtree(\"twitter/\") \n",
        "  os.mkdir(\"twitter\")\n",
        "    "
      ],
      "metadata": {
        "id": "-deyALygUkWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.get_user(id=user_obj.data.id,\n",
        "                          user_fields=[\"created_at\",\"description\",\"location\",\n",
        "                                       \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\n",
        "                                       \"verified\",\"public_metrics\"])"
      ],
      "metadata": {
        "id": "TXXalb2fX40w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**MCDONALDS API PULL FOR FOLLOWERS AND DESCRIPTIONS**"
      ],
      "metadata": {
        "id": "m8npxHfWmu6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the below code stub to pull the follower IDs and write them to a file. \n",
        "\n",
        "handles = ['McDonalds']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit= 20):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count','like_count', 'description']\n",
        "    columns_2 = ['ID']\n",
        "    mcd = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    mcd_users = pd.DataFrame(followers_data[handle], columns = columns_2)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    mcd.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    mcd_users.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AQybX53VyMq",
        "outputId": "2f1a1a6a-5761-4cca-fadc-69ae075e705f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'FUJA6KK2Q4S1GZZZ'}\n",
            "{'result_count': 1000, 'next_token': '8093AHJA3GS1GZZZ', 'previous_token': 'RHRM0T7K5R3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'U6M981Q0EKRHGZZZ', 'previous_token': '7F9SCDM1SF3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'T9LD0T3GN8R1GZZZ', 'previous_token': '58S8SF75HB4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'RV1JNNAJ20R1GZZZ', 'previous_token': 'NCJ1S95L8N4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'RIJ2QU4SECQHGZZZ', 'previous_token': 'THN4PT5CTV4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'I7GRDILJKOQ1GZZZ', 'previous_token': 'LESCS438HJ5EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CLSP8TC0TSPHGZZZ', 'previous_token': '69POECE3B75UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '0V9FVA5MAOPHGZZZ', 'previous_token': '943UH04Q236EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'ISK4IH2CKCP1GZZZ', 'previous_token': 'USICRHQCL76EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'DR0GRDDAVOOHGZZZ', 'previous_token': 'EPO54P8NBN6UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'R7CHOGT2HOOHGZZZ', 'previous_token': '5O6C3BAU077EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'DHK2UEFERKO1GZZZ', 'previous_token': 'VD6LF5CHE77EEZZZ'}\n",
            "{'result_count': 999, 'next_token': 'F2RIDHI66SO1GZZZ', 'previous_token': 'E9QQ2S974B7UEZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 893 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '4EH416IISKNHGZZZ', 'previous_token': 'T1BIDAO5P77UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'U7MGV92TL4NHGZZZ', 'previous_token': 'NKAUF5M43B8EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'F91VH9LFSON1GZZZ', 'previous_token': 'E0KS1TMSAR8EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'D38VNQ2028N1GZZZ', 'previous_token': 'JA5A19SL3B8UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'FTFK8I3CASMHGZZZ', 'previous_token': 'VH5SML3PTR8UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'KN0SECMPJ8M1GZZZ', 'previous_token': 'QGDTQ1MUL39EEZZZ'}\n",
            "0:15:03.747301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mcd.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRnUJp3Obvuf",
        "outputId": "8d9a12af-e7e8-4530-dd17-f532efa1cd02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19999, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mcd_df = mcd[mcd[\"description\"].str.strip().str.len()>0]\n",
        "mcd_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtGLyekFb_vn",
        "outputId": "b726d1da-2062-4eea-f08d-cdccc7be9dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9269, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check any duplicates\n",
        "print((mcd_df.duplicated()).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc6ffpm-nIL4",
        "outputId": "48f155bd-4e80-464d-f8d9-62bbc8e497ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**BURGER KING API PULL FOR FOLLOWERS AND DESCRIPTIONS**"
      ],
      "metadata": {
        "id": "0P91SrUBm4QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handles = ['BurgerKing']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit= 20):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count','like_count', 'description']\n",
        "    columns_2 = ['ID']\n",
        "    bgk = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    bgk_users = pd.DataFrame(followers_data[handle], columns = columns_2)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    bgk.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    bgk_users.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0J3G3nHm8y9",
        "outputId": "fe0abce4-eb38-403d-bc62-9aad25537535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '6QSS6CLEGGSHGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'T4SNT93NAGSHGZZZ', 'previous_token': 'RM4K5PQVFF3EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'G7CHGS6358SHGZZZ', 'previous_token': 'SSR866SILF3EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '0G4CTKC4V4S1GZZZ', 'previous_token': '7MNMFQPVQN3EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'DO686IFNP8S1GZZZ', 'previous_token': 'IS5ROVC00R3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CLIVT30RJ8S1GZZZ', 'previous_token': '0JN1GNGN6N3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'D1G2GVC9BSS1GZZZ', 'previous_token': '9MQGE3V5CN3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '724LPOB54CS1GZZZ', 'previous_token': '44FC2CRTK33UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'QR25KGSDV0RHGZZZ', 'previous_token': 'M670AIKSRJ3UEZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 508 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '1VRB2GO8P8RHGZZZ', 'previous_token': 'L9N9NSC30V4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '9FCFVTETJKRHGZZZ', 'previous_token': 'MT8NIBGA6R4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'ST8KS8CMBSRHGZZZ', 'previous_token': 'F85I82P2CB4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'ANSIVS9O40RHGZZZ', 'previous_token': 'AP013E3EK34EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '024Q62UITCR1GZZZ', 'previous_token': 'SKGJU8MSRV4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '5OI3B3KTO8R1GZZZ', 'previous_token': 'CQNIC31S2J4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '36IR9TDLJ0R1GZZZ', 'previous_token': '6SORCNBF7N4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '3P1I7DDGD8R1GZZZ', 'previous_token': 'NQSL2GIACV4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'PTJHOACT80R1GZZZ', 'previous_token': '7RM32UIUIN4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'GTDAFENQ2KR1GZZZ', 'previous_token': '59LPDRR8NV4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'RM14RB26SSQHGZZZ', 'previous_token': '3L31VA09TB4UEZZZ'}\n",
            "0:08:38.475400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bgk_df = bgk[bgk[\"description\"].str.strip().str.len()>0]\n",
        "bgk_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMK4vHyYqOKs",
        "outputId": "0b2adf62-830f-4e9c-c560-bd21e1dca9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7011, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check any duplicates\n",
        "print((bgk_df.duplicated()).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUL31pgPqV7f",
        "outputId": "a726e0df-4cb9-4389-f55b-899d24dc9296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**WENDYS API PULL FOR FOLLOWERS AND DESCRIPTIONS**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EYLzjTjCqsPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handles = ['Wendys']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit= 20):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count','like_count', 'description']\n",
        "    columns_2 = ['ID']\n",
        "    wd = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    wd_users = pd.DataFrame(followers_data[handle], columns = columns_2)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    wd.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    wd_users.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FMHbEOZqXs8",
        "outputId": "f37688b6-a640-4d56-95a1-7eb9f969933e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'S61KTDUNQKS1GZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'LF4IOIDH3KS1GZZZ', 'previous_token': 'DLSN8TT95B3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'D1E4EP37F0RHGZZZ', 'previous_token': '8G8H90QQSB3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'N3CHBGLFNCR1GZZZ', 'previous_token': 'NUSPG5LKGV4EEZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 368 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '7N58IN7S24R1GZZZ', 'previous_token': '5V3KR1348J4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '9J5JIQ7DEGQHGZZZ', 'previous_token': '2R1Q219STR4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'GGL2589AL8Q1GZZZ', 'previous_token': 'VGC3EMAAHF5EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'B3F3P6JGU4PHGZZZ', 'previous_token': '18R72PNNAN5UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '5L20U5V7B0PHGZZZ', 'previous_token': 'CM92A2F61R6EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'QJ7IP39EKKP1GZZZ', 'previous_token': '01GQF3QRKV6EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'HJJRLKMLVSOHGZZZ', 'previous_token': '9OGPKA6IBB6UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '4V4T07NNHSOHGZZZ', 'previous_token': 'KDDE45B3037EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'LM788O6KS0O1GZZZ', 'previous_token': 'F6LGHIPFE37EEZZZ'}\n",
            "{'result_count': 999, 'next_token': 'C8FBNLEA70O1GZZZ', 'previous_token': '80O5NOQP3V7UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6SG76UGRSONHGZZZ', 'previous_token': '6E90LM4KOV7UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'Q081R6UQLCNHGZZZ', 'previous_token': 'VCD26476378EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'NQJ6GT9ET4N1GZZZ', 'previous_token': 'T8FVVKSQAJ8EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'L20OJABF2KN1GZZZ', 'previous_token': '46OKAFH52V8UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'S30MJH7FB4MHGZZZ', 'previous_token': 'F8CSTLVOTB8UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '4QV07D4TJKM1GZZZ', 'previous_token': 'UBRIC517KR9EEZZZ'}\n",
            "0:06:17.562572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wd_df = wd[wd[\"description\"].str.strip().str.len()>0]\n",
        "wd_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPW5ZMI5ths4",
        "outputId": "369fe0f5-ac00-477d-fc10-7e1dc2527690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9271, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**In-N-Out Burger API PULL FOR FOLLOWERS AND DESCRIPTIONS**\n"
      ],
      "metadata": {
        "id": "_7fKgD5Rtotv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handles = ['innoutburger']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit= 20):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count','like_count', 'description']\n",
        "    columns_2 = ['ID']\n",
        "    inn = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    inn_users = pd.DataFrame(followers_data[handle], columns = columns_2)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    inn.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    inn_users.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mux5cofstx56",
        "outputId": "c4c18a00-141b-47af-9b72-7852cf2fe355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 649 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'G2LKH2O32CSHGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'BSFO4O5JIGS1GZZZ', 'previous_token': 'M4SOJF85TN3EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '3NQOS72C2GS1GZZZ', 'previous_token': 'RNBVAS2EDF3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'D6O3B5H9N8RHGZZZ', 'previous_token': 'C0QIOM66TF3UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '68F4CP543GRHGZZZ', 'previous_token': 'GBC8GB7B8N4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '9BUL27QDIKR1GZZZ', 'previous_token': '70PAK2BASF4EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '2RL47B5J6OR1GZZZ', 'previous_token': 'FJQL6D68DB4UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'JNCP200VSGQHGZZZ', 'previous_token': '40HPHQJ8P74UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'VCG86RC0GOQHGZZZ', 'previous_token': '9H1T8PNE3F5EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'GE6TIVLO2GQHGZZZ', 'previous_token': 'GSB21AKDF75EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6OO7LQH1G0Q1GZZZ', 'previous_token': '4215623ETF5EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'R1R035V13CQ1GZZZ', 'previous_token': 'AHM1R0NPFV5UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6CV71QI7Q8PHGZZZ', 'previous_token': 'G36SDDHMSJ5UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'FQIEQHETF0PHGZZZ', 'previous_token': 'SJHQS46T5N6EEZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 893 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': '7ULDD3531SPHGZZZ', 'previous_token': '2RVV2SH6GV6EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'UB4DL52HKSP1GZZZ', 'previous_token': '6BK4KPKVU36EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CFLTCQKT98P1GZZZ', 'previous_token': '56BN0RMHB36UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'P7LJQ7DIUOOHGZZZ', 'previous_token': 'NPDLLFTSMN6UEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'B0SF7VURM0OHGZZZ', 'previous_token': '5G7NT4QL177EEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'S6DN6EQJHGOHGZZZ', 'previous_token': '1JSK5QHG9V7EEZZZ'}\n",
            "0:25:52.421812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inn_df = inn[inn[\"description\"].str.strip().str.len()>0]\n",
        "inn_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8_0NHvV1ZiS",
        "outputId": "888fa27f-1190-4545-bb6a-726abefe3cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9877, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**JACK IN THE BOX API PULL FOR FOLLOWERS AND DESCRIPTIONS"
      ],
      "metadata": {
        "id": "Atp0JL7j1ldH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "handles = ['JackBox']\n",
        "user_obj = client.get_user(username=handle)\n",
        "handle_id = user_obj.data.id\n",
        "\n",
        "whitespace_pattern = re.compile(r\"\\s+\")\n",
        "\n",
        "user_data = dict() \n",
        "followers_data = dict()\n",
        "\n",
        "for handle in handles :\n",
        "    user_data[handle] = [] # will be a list of lists\n",
        "    followers_data[handle] = [] # will be a simple list of IDs\n",
        "\n",
        "\n",
        "# Grabs the time when we start making requests to the API\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for handle in handles :\n",
        "    \n",
        "    # Create the output file names \n",
        "    \n",
        "    followers_output_file = handle + \"_followers.txt\"\n",
        "    user_data_output_file = handle + \"_follower_data.txt\"\n",
        "    \n",
        "    \n",
        "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
        "    # use `get_users_followers` to pull the follower data requested. \n",
        "    \n",
        "    for response in tweepy.Paginator(client.get_users_followers, handle_id,\n",
        "                                 user_fields = [\"id\",\"username\",\"name\",\"location\",\"public_metrics\",\"description\"],\n",
        "                                    max_results=1000, limit= 20):\n",
        "      print(response.meta)\n",
        "      \n",
        "      for follower in response.data :\n",
        "        follower_row = {\n",
        "        'ID' : follower.id,\n",
        "        'screen_name' : follower.username,\n",
        "        'name' : follower.name,\n",
        "        'location' : follower.location,\n",
        "        'follower_count' : follower.public_metrics['followers_count'],\n",
        "        'friends_count' : follower.public_metrics['following_count'],\n",
        "        'description': re.sub(r\"\\s+\",\" \", follower.description).strip(),\n",
        "        }\n",
        "        \n",
        "        user_data[handle].append(follower_row)   # All lists\n",
        "        followers_data[handle].append(follower.id)  # just IDs\n",
        "  \n",
        "  #Save the extracted followers to a dataframe\n",
        "    columns = ['ID', 'screen_name', 'name', 'location', 'follower_count', 'friends_count','like_count', 'description']\n",
        "    columns_2 = ['ID']\n",
        "    jkb = pd.DataFrame(user_data[handle], columns=columns)\n",
        "    jkb_users = pd.DataFrame(followers_data[handle], columns = columns_2)\n",
        "  \n",
        "  \n",
        "  #Write to the output file\n",
        "    jkb.to_csv(f'twitter/{handle}_followers.txt', sep = '\\t', index = False)\n",
        "    jkb_users.to_csv(f'twitter/{handle}_follower_data.txt', sep = '\\t', index = False)\n",
        "     \n",
        "\n",
        "# Let's see how long it took to grab all follower IDs\n",
        "end_time = datetime.datetime.now()\n",
        "print(end_time - start_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chqe0avF1j4Y",
        "outputId": "a4de7f7c-cf68-4783-aa60-388dc3f849d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'P2SI51GR60HHGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'N9NJSCACGFVHEZZZ', 'previous_token': 'MTAA0J7EPVEEEZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'A3R25PNLON81EZZZ', 'previous_token': 'V0GLV3L8FO0EGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'KLH7FS4D9V51EZZZ', 'previous_token': 'O913OU0R7KNUGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'LNOG027QRJ4HEZZZ', 'previous_token': 'FQIT077RM0QUGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'S4IB8UO30II1EZZZ', 'previous_token': 'QBGLCH874CREGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '95JQIOSA8HEHEZZZ', 'previous_token': '9VVJANUT19EEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6NUGI6P4RKJHEZZZ', 'previous_token': 'M99MD7RONEHEGZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'CV3R0UTNONN1CZZZ', 'previous_token': 'JDF3J3FF7BCEGZZZ'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 373 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'result_count': 1000, 'next_token': 'RQIOIN0U3EUHCZZZ', 'previous_token': 'P231GPAFF08UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '48SIQ8AK0IBHCZZZ', 'previous_token': '8G89GMK7051UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'S4QGHCAL75U1CZZZ', 'previous_token': 'AQ3S81SQ2LKUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'T06EGQ82U9EHCZZZ', 'previous_token': 'DGCF7P0HOU1UIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'S7KH2E4R2DCHCZZZ', 'previous_token': 'NJRDTCVK32HEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'RGQA7SV7NPC1CZZZ', 'previous_token': 'TEFNC7BITIJEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'NRUJ0G7IMP0HCZZZ', 'previous_token': 'V75C711S86JUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'I9DIR91M6OK1CZZZ', 'previous_token': 'T4T4JOM296VEIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'NIR17071AC91CZZZ', 'previous_token': 'K7VAK1BEPBBUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': '6KMO6P5KTC01CZZZ', 'previous_token': 'C77TJAQGLRMUIZZZ'}\n",
            "{'result_count': 1000, 'next_token': 'PHUF4C3MC7K1AZZZ', 'previous_token': '1VTS9FS62JVUIZZZ'}\n",
            "0:06:24.000557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jkb_df = jkb[jkb[\"description\"].str.strip().str.len()>0]\n",
        "jkb_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEzxJJ2N3snY",
        "outputId": "9d3cb25f-3463-412f-a921-a439a355c570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13800, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check any duplicates\n",
        "print((bgk_df.duplicated()).sum())\n",
        "print((mcd_df.duplicated()).sum())\n",
        "print((inn_df.duplicated()).sum())\n",
        "print((jkb_df.duplicated()).sum())\n",
        "print((wd_df.duplicated()).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O4hzuB93432",
        "outputId": "7e7625b5-79d0-49c5-8d06-ab5a1a19acc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export as csv files\n",
        "mcd_df.to_csv('mcd.csv', index= False)\n",
        "wd_df.to_csv('wendy.csv',index=False)\n",
        "inn_df.to_csv('innout.csv',index=False)\n",
        "jkb_df.to_csv('jack.csv',index=False)\n",
        "bgk_df.to_csv('burgerk.csv', index=False)"
      ],
      "metadata": {
        "id": "PduwVJwS91oL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}